{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This is the initial network design for chromstem-net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the dataset generator class, to be loaded into the DataLoader tool\n",
    "\n",
    "This is adapted from the pytorch tutorial: https://pytorch.org/tutorials/beginner/data_loading_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromstemDataset(Dataset):\n",
    "    \"\"\"Chromstem Dataset\"\"\"\n",
    "    \n",
    "    def __init__(self,csv_file,root_dir,transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Master CSV file that will hold all the data\n",
    "            root_dir (string): Directory where the datasets are labeled\n",
    "            transform (callable, optional): Optional transformations to samples\n",
    "        \"\"\"\n",
    "        \n",
    "        f = open(csv_file,'r')\n",
    "        csv_length = len(f.readlines()[-1].split(','))\n",
    "        f.close()\n",
    "        col_names = ['File'] + ['Nucl%d' % i for i in range(csv_length-1)]\n",
    "        self.nucl_coords_frame_ = pd.read_csv(csv_file,engine='python',header=None,names=col_names)\n",
    "        self.root_dir_ = root_dir\n",
    "        self.transform_ = transform\n",
    "    \n",
    "    def read_chromstem(self,fnme):\n",
    "        x,y,z,rho = np.loadtxt(fnme,comments='#',unpack=True)\n",
    "        tensor = torch.zeros_like(torch.empty(1,int(x[0]),int(y[0]),int(z[0])))\n",
    "    \n",
    "        for i,arr in enumerate(zip(x[1:],y[1:],z[1:])):\n",
    "            tensor[0,int(arr[0]),int(arr[1]),int(arr[2])] = rho[i]\n",
    "        \n",
    "        # Convert to sparse matrix as data will be sparse\n",
    "        #tensor = torch.Sparse.FloatTensor(tensor)\n",
    "        return tensor\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(len(self.nucl_coords_frame_))\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        \n",
    "        chromstem_name = os.path.join(self.root_dir_,self.nucl_coords_frame_.iloc[idx,0])\n",
    "        chromstem = self.read_chromstem(chromstem_name)\n",
    "        nucl_coords = self.nucl_coords_frame_.iloc[idx,1:]\n",
    "        nucl_coords = np.asarray([nucl_coords])\n",
    "        nucl_coords = nucl_coords.astype('float').reshape(-1,3)\n",
    "        num_nucls = int(chromstem_name.split('/data/')[1].split('/')[0])\n",
    "        \n",
    "        sample = {'chromstem' : chromstem, 'nucl_coords' : nucl_coords, 'num_nucls' : num_nucls}\n",
    "        \n",
    "        if self.transform_:\n",
    "            sample = self.transform_(sample)\n",
    "            \n",
    "        return sample\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, would be useful to provide a plotting function for samples in the dataset.\n",
    "The main input to this function is any sample from the dataset and it would output the density and \"centromere\" in a 3D plot.\n",
    "To do so accurately, I am going to use the ipyvolume widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ipyvolume as ipv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample(sample):\n",
    "    ipv.figure()\n",
    "    # Plot voxels of density\n",
    "    ipv.scatter(sample['chromstem'][:,0],sample['chromstem'][:,1],sample['chromstem'][:,2], \n",
    "                color='blue',size=10,marker='box',opacity=0.5)\n",
    "    \n",
    "    # Plot nucleosome centers\n",
    "    ipv.scatter(sample['nucl_coords'][:,0],sample['nucl_coords'][:,1],sample['nucl_coords'][:,2],\n",
    "               color='red',size=5,marker='sphere')\n",
    "    \n",
    "    ipv.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a quick test of the widget and plotting function before the data is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4390f3e19c04828badc0f89df0e5332",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Figure(camera=PerspectiveCamera(fov=46.0, position=(0.0, 0.0, 2.0), quaternion=(0.0, 0.0, 0.0, â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = {'chromstem' :  np.asarray([[1,2,3]]), 'nucl_coords' : np.asarray([[0,1,2],[5,10,0.2],[10,3.3,2]])}\n",
    "plot_sample(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where the dataset is loaded in. The .csv file should be located in the current directory, so the root is '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromstem_dataset = ChromstemDataset(csv_file='output_label_test.csv',\n",
    "                                     root_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a quick test to make sure the dataset looks appropriate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'chromstem': tensor([[[[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 2.8299, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.5896, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.9433, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]],\n",
      "\n",
      "         [[0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          ...,\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000],\n",
      "          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]]), 'nucl_coords': array([[ 492.935  , -451.337  ,  480.476  ],\n",
      "       [  -8.76539,  468.342  , -468.397  ],\n",
      "       [-442.353  ,   11.2638 , -399.868  ],\n",
      "       [ -62.854  ,  -34.68   ,  485.72   ],\n",
      "       [-459.983  ,  476.427  ,  -36.7292 ]]), 'num_nucls': 5}\n"
     ]
    }
   ],
   "source": [
    "print(chromstem_dataset[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I will load in the entire dataset and begin to feed it into a built neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromstem_dataset = ChromstemDataset(csv_file='output_label.csv',\n",
    "                                     root_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, the dataloader will be able to shuffle the data, and I will start with a batch size of 4. No idea if that will work, but for now this is a fixed value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(chromstem_dataset, batch_size=1,\n",
    "                        shuffle=True, num_workers=1)\n",
    "\n",
    "testloader  = DataLoader(chromstem_dataset, batch_size=1,\n",
    "                         shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the neural network class will be initialized here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net,self).__init__()\n",
    "        self.conv1 = nn.Conv3d(1,16,3)\n",
    "        self.conv2 = nn.Conv3d(16,32,3)\n",
    "        self.pool = nn.MaxPool3d(3,3)\n",
    "        self.fc1   = nn.Linear(32*4*2,100)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, the cross entropy loss function is chosen, however a different function may be required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the user has CUDA functionality on their computer, the network trains significantly faster with GPU. As such, here I will provide the opportunity to use CUDA. A correct output would say that the device being used is \"cuda:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv3d(1, 16, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "  (conv2): Conv3d(16, 32, kernel_size=(3, 3, 3), stride=(1, 1, 1))\n",
       "  (pool): MaxPool3d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  (fc1): Linear(in_features=256, out_features=100, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "\n",
    "net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network briefly here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 3.955\n",
      "[1,   200] loss: 3.036\n",
      "[1,   300] loss: 2.663\n",
      "[1,   400] loss: 2.501\n",
      "[1,   500] loss: 2.061\n",
      "[1,   600] loss: 1.810\n",
      "[1,   700] loss: 1.603\n",
      "[1,   800] loss: 1.617\n",
      "[1,   900] loss: 1.409\n",
      "[1,  1000] loss: 1.226\n",
      "[1,  1100] loss: 1.357\n",
      "[1,  1200] loss: 1.106\n",
      "[1,  1300] loss: 0.981\n",
      "[1,  1400] loss: 0.878\n",
      "[1,  1500] loss: 1.026\n",
      "[1,  1600] loss: 0.861\n",
      "[1,  1700] loss: 0.757\n",
      "[1,  1800] loss: 0.759\n",
      "[1,  1900] loss: 0.723\n",
      "[1,  2000] loss: 0.912\n",
      "[1,  2100] loss: 0.873\n",
      "[1,  2200] loss: 0.746\n",
      "[1,  2300] loss: 0.726\n",
      "[1,  2400] loss: 0.837\n",
      "[1,  2500] loss: 0.664\n",
      "[1,  2600] loss: 0.582\n",
      "[1,  2700] loss: 0.882\n",
      "[1,  2800] loss: 0.653\n",
      "[1,  2900] loss: 0.709\n",
      "[1,  3000] loss: 0.504\n",
      "[1,  3100] loss: 0.768\n",
      "[1,  3200] loss: 0.651\n",
      "[1,  3300] loss: 0.666\n",
      "[1,  3400] loss: 0.482\n",
      "[1,  3500] loss: 0.401\n",
      "[1,  3600] loss: 0.517\n",
      "[1,  3700] loss: 0.628\n",
      "[1,  3800] loss: 0.445\n",
      "[1,  3900] loss: 0.544\n",
      "[1,  4000] loss: 0.639\n",
      "[1,  4100] loss: 0.469\n",
      "[1,  4200] loss: 0.686\n",
      "[1,  4300] loss: 0.456\n",
      "[1,  4400] loss: 0.556\n",
      "[1,  4500] loss: 0.683\n",
      "[1,  4600] loss: 0.330\n",
      "[1,  4700] loss: 0.335\n",
      "[1,  4800] loss: 0.315\n",
      "[1,  4900] loss: 0.635\n",
      "[1,  5000] loss: 0.605\n",
      "[1,  5100] loss: 0.346\n",
      "[1,  5200] loss: 0.384\n",
      "[1,  5300] loss: 0.540\n",
      "[1,  5400] loss: 0.379\n",
      "[1,  5500] loss: 0.462\n",
      "[1,  5600] loss: 0.310\n",
      "[1,  5700] loss: 0.412\n",
      "[1,  5800] loss: 0.556\n",
      "[1,  5900] loss: 0.361\n",
      "[1,  6000] loss: 0.432\n",
      "[1,  6100] loss: 0.344\n",
      "[1,  6200] loss: 0.638\n",
      "[1,  6300] loss: 0.391\n",
      "[1,  6400] loss: 0.442\n",
      "[1,  6500] loss: 0.432\n",
      "[1,  6600] loss: 0.694\n",
      "[1,  6700] loss: 0.247\n",
      "[1,  6800] loss: 0.434\n",
      "[1,  6900] loss: 0.351\n",
      "[1,  7000] loss: 0.361\n",
      "[1,  7100] loss: 0.485\n",
      "[1,  7200] loss: 0.328\n",
      "[1,  7300] loss: 0.642\n",
      "[1,  7400] loss: 0.432\n",
      "[1,  7500] loss: 0.323\n",
      "[1,  7600] loss: 0.824\n",
      "[1,  7700] loss: 0.393\n",
      "[1,  7800] loss: 0.272\n",
      "[1,  7900] loss: 0.453\n",
      "[1,  8000] loss: 0.256\n",
      "[1,  8100] loss: 0.258\n",
      "[1,  8200] loss: 0.206\n",
      "[1,  8300] loss: 0.137\n",
      "[1,  8400] loss: 0.402\n",
      "[1,  8500] loss: 0.426\n",
      "[1,  8600] loss: 0.295\n",
      "[1,  8700] loss: 0.396\n",
      "[1,  8800] loss: 0.238\n",
      "[1,  8900] loss: 0.341\n",
      "[1,  9000] loss: 0.344\n",
      "[1,  9100] loss: 0.503\n",
      "[1,  9200] loss: 0.420\n",
      "[1,  9300] loss: 0.254\n",
      "[1,  9400] loss: 0.438\n",
      "[1,  9500] loss: 0.140\n",
      "[1,  9600] loss: 0.226\n",
      "[1,  9700] loss: 0.439\n",
      "[1,  9800] loss: 0.438\n",
      "[1,  9900] loss: 0.215\n",
      "[1, 10000] loss: 0.368\n",
      "[1, 10100] loss: 0.227\n",
      "[1, 10200] loss: 0.190\n",
      "[1, 10300] loss: 0.285\n",
      "[1, 10400] loss: 0.190\n",
      "[1, 10500] loss: 0.243\n",
      "[1, 10600] loss: 0.294\n",
      "[1, 10700] loss: 0.335\n",
      "[1, 10800] loss: 0.458\n",
      "[1, 10900] loss: 0.412\n",
      "[1, 11000] loss: 0.277\n",
      "[1, 11100] loss: 0.355\n",
      "[1, 11200] loss: 0.184\n",
      "[1, 11300] loss: 0.301\n",
      "[1, 11400] loss: 0.221\n",
      "[1, 11500] loss: 0.300\n",
      "[1, 11600] loss: 0.282\n",
      "[1, 11700] loss: 0.485\n",
      "[1, 11800] loss: 0.167\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-100-9927a180f9ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m99\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# print every 100 mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             print('[%d, %5d] loss: %.3f' %\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data['chromstem'].to(device), data['num_nucls'].to(device)\n",
    "    \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        _,preds = torch.max(outputs,1)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 100 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With two convolutional layers of 1->16 and 16->32 channels and one output fully-connected layer, the network is able to learn the number of nucleosomes to some degree. I will now switch to python scripts instead of notebook for a more fleshed-out workflow since the preliminary work is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:chromstem] *",
   "language": "python",
   "name": "conda-env-chromstem-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
